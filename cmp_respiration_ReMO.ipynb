{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating respiration from BGC-Argo data\n",
    "## Introduction\n",
    "In this notebook we will exploit oxygen data from BGC-Argo floats to estimate respiration in the mesopelagic ocean. The method we will use is the one described in Martz et al. ([2008](https://aslopubs.onlinelibrary.wiley.com/doi/abs/10.4319/lo.2008.53.5_part_2.2094)) an Hennon et al. ([2016](http://krill.ocean.washington.edu/riser_web/Hennon_et_al-2016-Global_Biogeochemical_Cycles.published.pdf)), although we will also add some modifications. <br>\n",
    "Briefly, the method is based on quantifying the time rate of change of oxygen in the mesopelagic (i.e., below the \"productive layer\", where oxygen can be produced by photosynthesis). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to make the Jupyter notebook as wide as the screen\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "These are the Python packags we use for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import hvplot.xarray\n",
    "import matplotlib.dates as mdates\n",
    "import pdb\n",
    "\n",
    "import bgc_tools # this is a series of routines used to manipulate BGC-Argo data\n",
    "import cmocean as cm# some colormaps\n",
    "from matplotlib.cm import get_cmap # https://colorcet.holoviz.org/user_guide/\n",
    "\n",
    "import copy # this is to create a deep copy of a dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "The first step is to read data from the float we want to use to estimate respiration.\n",
    "Two pieces of information are needed: the World Meteorological Organisation (WMO) number that identifies the specific float we want to work with and the acronym of the Argo Data Assembly Centre (DAC) that has acquired and pre-processed the data from this float. <br>\n",
    "\n",
    "First, select the float you want to work with using [this website](https://fleetmonitoring.euro-argo.eu/dashboard?Status=Active,Inactive&Year%20of%20deployment=2021,2020,2019,2018,2017,2016,2014,2015,2013,2012,2010,2011,2009&Variable=OXYGEN) (or others you may be more familiar with) and enter its WMO and DAC in the cell below. Note that I have already selected which deployment years to use and that we are looking for floats that have measured oxygen (\"DOXY\" in Argo terminology). <br>\n",
    "\n",
    "You will need a float that has at least one year of data, so not all the floats that will appear on the map will be suitable. In the cell below, I have already selected a float with enough data and signal, so you can start with this. After you have become familiar with the code, you are welcome to select other floats and try again. <br>\n",
    "\n",
    "To read the data we will use the Argo [THREDDS server](https://www.unidata.ucar.edu/software/tds/), that is a web server that allows us to download the data we need, when we need them. This is convenient because we do not need to have the entire BGC-Argo dataset in our computer. Because the file is not on your computer and it needs to be downloaded, it may take a while to run the cell below.<br> \n",
    "\n",
    "Among the Argo files, we will work with the synthetic-profile (Sprof) file for each float. The Sprof file contains the core Argo variables as well as all the BGC-Argo variables that have been co-located to common pressure levels. If you want to learn how the Sprof file is generated, please read [this document](https://archimer.ifremer.fr/doc/00445/55637/#). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## enter DAC and WMO below\n",
    "# below are some examples of floats, but you can enter whatever float you want (as long as it has DOXy data for at least one year)\n",
    "# DAC = 'coriolis'; WMO = '6902807' # N Atlantic\n",
    "# DAC = 'aoml'; WMO = '1900722' # Hennon\n",
    "# DAC = 'coriolis'; WMO = '6901573' # Red Sea, but does not seem to have PSAL_ADJUSTED and TEMP_ADJUSTED\n",
    "# DAC = 'coriolis'; WMO = '6903574' # Nordic Seas\n",
    "# DAC = 'csio'; WMO = '2902755' # NW Pacific\n",
    "# DAC = 'coriolis'; WMO = '6903250' # South Adriatic Sea\n",
    "# DAC = 'aoml'; WMO = '5904679' # S Ocean\n",
    "DAC = 'aoml'; WMO = '5904183' # S Ocean (under ice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### open file using Argo THREDDS server ####\n",
    "THREDDS_string = \"https://tds0.ifremer.fr/thredds/dodsC/CORIOLIS-ARGO-GDAC-OBS/\" # this is the main web address of Argo THREDDS server\n",
    "\n",
    "FN = THREDDS_string + DAC + \"/\" + WMO + \"/\" + WMO + \"_Sprof.nc\" # here we create the full string pointing to the specific file on the Argo THREDDS server\n",
    "ds = xr.open_dataset(FN) # here is where Xarray does the magic and downloads the file for you\n",
    "ds # visualize the content of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argo NetCDF file structure and dimensions\n",
    "Note above that the standard Argo NetCDF file uses N_PROF (i.e., profile number, related to time) and N_LEVELS (profile level, related to depth/pressure) as dimensions for the oceanographic variables (e.g., temperature, \"TEMP\").\n",
    "This is slightly cumbersome for manipulating data, but is needed because each profile in the Sprof file has a different pressure values.\n",
    "It would be easier to work with data the if each profile had the same pressure. We can achieve this by interpolating all data we are interested in onto a common pressure axis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate float data to the same pressure\n",
    "This is ensure each variable is on the same pressure axis and that we can use pressure (i.e., \"PRES\") as a coordinate in the Xarray Dataset. The latter is convenient for manipulating the data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create a common pressure axis\n",
    "NEW_PRES = np.arange(0, np.nanmax(ds.PRES.values), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### now interpolate the variables we want to work with to the common pressure axis\n",
    "\n",
    "# these are the variables we are using\n",
    "if ('PRES_ADJUSTED' in ds.keys()):\n",
    "    if (not np.all(np.isnan(ds.PRES_ADJUSTED.values))):\n",
    "        P = ds.PRES_ADJUSTED.values # I assume the ADJUSTED variable is available\n",
    "    else:\n",
    "        P = ds.PRES.values\n",
    "\n",
    "if ('PSAL_ADJUSTED' in ds.keys()):\n",
    "    if (not np.all(np.isnan(ds.PSAL_ADJUSTED.values))):\n",
    "        S = ds.PSAL_ADJUSTED.values # I assume the ADJUSTED variable is available\n",
    "    else:\n",
    "        S = ds.PSAL.values\n",
    "        \n",
    "if ('TEMP_ADJUSTED' in ds.keys()):\n",
    "    if (not np.all(np.isnan(ds.TEMP_ADJUSTED.values))):\n",
    "        T = ds.TEMP_ADJUSTED.values # I assume the ADJUSTED variable is available\n",
    "    else:\n",
    "        T = ds.TEMP.values\n",
    "\n",
    "if ('DOXY_ADJUSTED' in ds.keys()):\n",
    "    if (not np.all(np.isnan(ds.DOXY_ADJUSTED.values))):\n",
    "        DOXY = ds.DOXY_ADJUSTED.values # I assume the ADJUSTED variable is available\n",
    "    else:\n",
    "        DOXY = ds.DOXY.values\n",
    "        \n",
    "\n",
    "# initialise arrays to NaN values for the new interpolated variables\n",
    "Si = np.empty((ds.JULD.shape[0], NEW_PRES.shape[0])) + np.nan\n",
    "Ti = np.empty((ds.JULD.shape[0], NEW_PRES.shape[0])) + np.nan\n",
    "DOXYi = np.empty((ds.JULD.shape[0], NEW_PRES.shape[0])) + np.nan\n",
    "\n",
    "i_allNaNs = []\n",
    "\n",
    "for it, tmp in enumerate(ds.JULD.values): # loop through each date/profile\n",
    " \n",
    "    # store position of where are data are all NaNs\n",
    "    if np.all(np.isnan(DOXY[it,:])):\n",
    "        i_allNaNs.append(it)\n",
    "\n",
    "\n",
    "    #PSAL\n",
    "    innan = np.where((~np.isnan(P[it,:])) & (~np.isnan(S[it,:])))[0]# find non-NaN values\n",
    "    Stmp = S[it,:][innan] # and create a vector only with real numbers\n",
    "    Ptmp = P[it,:][innan] # and create a vector only with real numbers   \n",
    "    \n",
    "    if ~np.all(np.diff(Ptmp)>0): # if Ptmp is not monotonically increasing (needed by np.interp), then stop here\n",
    "        pdb.set_trace()\n",
    "    if (np.all(np.isnan(Stmp))): # check if we have only NaNs and, if so, go to the next iteration\n",
    "        continue\n",
    "        \n",
    "    Si[it,:] = np.interp(NEW_PRES, Ptmp, Stmp) # interpolate variable to NEW_PRES axis\n",
    "    \n",
    "\n",
    "    #TEMP\n",
    "    innan = np.where((~np.isnan(P[it,:])) & (~np.isnan(T[it,:])))[0]# find non-NaN values\n",
    "    Ttmp = T[it,:][innan] # and create a vector only with real numbers\n",
    "    Ptmp = P[it,:][innan] # and create a vector only with real numbers   \n",
    "\n",
    "    if ~np.all(np.diff(Ptmp)>0): # if Ptmp is not monotonically increasing (needed by np.interp), then stop here\n",
    "        pdb.set_trace()\n",
    "        \n",
    "    if (np.all(np.isnan(Ttmp))): # check if we have only NaNs and, if so, go to the next iteration\n",
    "        continue\n",
    "        \n",
    "    Ti[it,:] = np.interp(NEW_PRES, Ptmp, Ttmp) # interpolate variable to NEW_PRES axis\n",
    "    \n",
    "\n",
    "    #DOXY\n",
    "    innan = np.where((~np.isnan(P[it,:])) & (~np.isnan(DOXY[it,:])))[0]# find non-NaN values\n",
    "    DOXYtmp = DOXY[it,:][innan] # and create a vector only with real numbers\n",
    "    Ptmp = P[it,:][innan] # and create a vector only with real numbers   \n",
    "    \n",
    "    if ~np.all(np.diff(Ptmp)>0): # if Ptmp is not monotonically increasing (needed by np.interp), then stop here\n",
    "        pdb.set_trace()\n",
    "        \n",
    "    if (np.all(np.isnan(DOXYtmp))): # check if we have only NaNs and, if so, go to the next iteration\n",
    "        continue\n",
    "        \n",
    "    DOXYi[it,:] = np.interp(NEW_PRES, Ptmp, DOXYtmp) # interpolate variable to NEW_PRES axis\n",
    "    \n",
    "    \n",
    "# remove profiles that are all NaNs\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new Xarray Dataset\n",
    "We will now store the interpolated variables inside the \"Dataset object\" of Xarray. This is useful because Xarray will allow us to easily manipulate and visualize the data. <br>\n",
    "If you want to learn more about Xarray, see the [official documentation](https://docs.xarray.dev/en/stable/) and this [introductory tutorial](https://earth-env-data-science.github.io/lectures/xarray/xarray_intro.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Now create new Xarray Dataset\n",
    "if \"ds_new\" in locals(): # delete ds_new if it exists already\n",
    "    del ds_new\n",
    "    \n",
    "\n",
    "## first create the DataArrays that will be part of the dataset\n",
    "da_Si = xr.DataArray(  Si, \n",
    "                       dims = ['JULD', 'PRES'], # note the new dimensions\n",
    "                       coords = {'JULD': ds.JULD.values, # note the new coordinates\n",
    "                               'PRES': NEW_PRES\n",
    "                              },\n",
    "                       attrs = ds.PSAL.attrs # here I am copying the attributes that contain metadata about the variable (e.g., units) )\n",
    "                    )\n",
    "\n",
    "da_Ti = xr.DataArray(Ti, \n",
    "                       dims = ['JULD', 'PRES'],\n",
    "                       coords = {'JULD': ds.JULD.values,\n",
    "                               'PRES': NEW_PRES\n",
    "                              },\n",
    "                       attrs = ds.TEMP.attrs\n",
    "                    )\n",
    "\n",
    "da_DOXYi = xr.DataArray(DOXYi, \n",
    "                       dims = ['JULD', 'PRES'],\n",
    "                       coords = {'JULD': ds.JULD.values,\n",
    "                               'PRES': NEW_PRES\n",
    "                              },\n",
    "                       attrs = ds.DOXY.attrs\n",
    "                    )\n",
    "\n",
    "# create new dataset from the salinity DataArray\n",
    "ds_new = da_Si.rename(\"PSAL\").to_dataset()\n",
    "\n",
    "# add temperature to the new dataset\n",
    "ds_new[\"TEMP\"] = da_Ti\n",
    "\n",
    "# add oxygen to the new dataset\n",
    "ds_new[\"DOXY\"] = da_DOXYi\n",
    "\n",
    "# add NO3 to the new dataset, if available\n",
    "if 'NITRATE' in ds_new.keys():\n",
    "\n",
    "    da_DNO3i = xr.DataArray(DOXYi, \n",
    "                           dims = ['JULD', 'PRES'],\n",
    "                           coords = {'JULD': ds.JULD.values,\n",
    "                                   'PRES': NEW_PRES\n",
    "                                  },\n",
    "                           attrs = ds.DOXY.attrs\n",
    "                        )\n",
    "    ds_new[\"NO3\"] = da_DOXYi\n",
    "\n",
    "# add numeric decimal day since float deployment\n",
    "ds_new[\"decimal_day\"] = ds_new['JULD'].copy(deep=True)\n",
    "decimal_day = (ds_new.JULD.values.astype(float) - ds_new.JULD.values.astype(float)[0])/1e9/24/60/60\n",
    "ds_new[\"decimal_day\"] = xr.DataArray( decimal_day, \n",
    "                                       dims=['JULD'],\n",
    "                                       coords={'JULD': ds.JULD.values,\n",
    "                                      },\n",
    "                                       attrs = {'units': 'days',\n",
    "                                                'long_name': 'Time since deployment',\n",
    "                                               }\n",
    "                                    )\n",
    "\n",
    "# now add geographic coordinates as well\n",
    "ds_new.coords['LONGITUDE'] = ('JULD', ds.LONGITUDE.values)\n",
    "ds_new.coords['LATITUDE'] = ('JULD', ds.LATITUDE.values)\n",
    "\n",
    "# add attributes for coordinates\n",
    "ds_new['PRES'].attrs = ds.PRES.attrs\n",
    "# ds_new['JULD'].attrs = ds.JULD.attrs # I'm not adding these because they are not what we need\n",
    "ds_new['LONGITUDE'].attrs = ds.LONGITUDE.attrs\n",
    "ds_new['LATITUDE'].attrs = ds.LATITUDE.attrs\n",
    "\n",
    "\n",
    "\n",
    "## remove columns (dates) with all(DOXY==NaN), if present\n",
    "if i_allNaNs:\n",
    "    ds_new = ds_new.drop_isel(JULD=i_allNaNs)\n",
    "\n",
    "# visualise the new Xarray Dataset\n",
    "ds_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above that the new Xarray Dataset has different dimensions and coordinates than the standard Argo Sprof file and, importantly, that the second dimension is pressure (\"PRES\").\n",
    "Note also that Xarray has automatically generated two indexes from the new coordinates: these will be useful for exploiting the power of Xarray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute derived variables\n",
    "We now compute a new set of derived variables that we will need for our analysis from the exising Argo variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute potential density anomaly\n",
    "ds_new = bgc_tools.cmp_sigma(ds_new)\n",
    "\n",
    "# compute mixed-layer depth\n",
    "ds_new = bgc_tools.cmp_zm_interpolated_data(ds_new)\n",
    "\n",
    "# compute depth of euphotic zone (works only if CHLA is available as well)\n",
    "ds_new = bgc_tools.cmp_zeu(ds_new)\n",
    "\n",
    "# compute depth of productive layer\n",
    "ds_new = bgc_tools.cmp_zp(ds_new)\n",
    "\n",
    "# compute DOXY_SATURATION and AOU\n",
    "ds_new = bgc_tools.cmp_o2sat_aou(ds_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### display the newly created variables\n",
    "ds_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot data\n",
    "We now begin by plotting some of the data we will be working with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's see where the float was\n",
    "The locations of the float profiles is plotted in magenta in the map below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a small map showing the location of the float\n",
    "bgc_tools.plot_map(ds_new);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next let's visualise the float trajectory in more detail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(4,3)) \n",
    "\n",
    "ds_new.plot.scatter(x='LONGITUDE', y='LATITUDE', \n",
    "                    hue='JULD', \n",
    "                    linewidth=1,\n",
    "                    s=20,\n",
    "                    edgecolor='k',\n",
    "                    cmap='RdPu',                    \n",
    "                    ax=ax,\n",
    "                    zorder=2,\n",
    "\n",
    "                   )\n",
    "ax.plot(ds_new.LONGITUDE.values, ds_new.LATITUDE.values, 'k-', zorder=1)\n",
    "\n",
    "# # improve formatting of colorbar ticklables (try commenting this out to see what happens otherwise)\n",
    "cb = fig.axes[-1] # this is the colorbar axis handle\n",
    "old = cb.get_ymajorticklabels() # read the existing ticklabels\n",
    "N = 6 # approximate number of labels for the colorbar\n",
    "nn = int(len(old)/N) # integer number of subset of ticklabels\n",
    "pstns = [i.get_position()[-1] for i in old[0:-1:nn] ] # positions of the subset of ticklables\n",
    "lbls = [i.get_text()[:10] for i in old[0:-1:nn]] # labels of the subset of ticklabels\n",
    "cb.set_yticks(pstns, labels=lbls); # set the new ticks and corresponding lables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now plot the oceanographic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(14,3)) # prepare figure and axis objects\n",
    "ds_new.zm.plot(c='pink', ax=ax, zorder=2, label=\"$z_m$\") # plot zm\n",
    "ds_new.TEMP.plot(y='PRES', cmap='magma', vmin=-2, vmax=3, ax=ax, zorder=1) # plot temperature\n",
    "ds_new.sigma0.plot.contour(y=\"PRES\", levels=45, colors='w', ax=ax, alpha=0.5, linewidths=0.5) # plot density anomalies\n",
    "ax.legend()\n",
    "ax.grid('on', ls='--', lw=0.5, alpha=0.5)\n",
    "ax.set_ylim([600, 0]); # set limits and *direction* of yaxis range\n",
    "# ax.set_xlim([np.datetime64(\"2007-07-01\"), np.datetime64(\"2008-07-01\")]); # how to set xlim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(14,3)) # prepare figure and axis objects\n",
    "ds_new.zm.plot(c='k', ax=ax, zorder=2) # plot zm\n",
    "ds_new.PSAL.plot(y='PRES', cmap=cm.cm.haline, ax=ax, zorder=1, vmin=33.5, vmax=35) # plot oxygen\n",
    "ds_new.sigma0.plot.contour(y=\"PRES\", levels=45, colors='k', ax=ax, alpha=0.5, linewidths=0.5) # plot density anomalies\n",
    "\n",
    "ax.set_ylim([600, 0]); # set limits and **direction** of yaxis range\n",
    "# ax.set_xlim([np.datetime64(\"2007-07-01\"), np.datetime64(\"2008-07-01\")]); # how to set xlim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(14,3)) # prepare figure and axis objects\n",
    "ds_new.zm.plot(c='k', ax=ax, zorder=2) # plot zm\n",
    "ds_new.DOXY.plot(y='PRES', cmap=get_cmap('cet_bmy'), ax=ax, zorder=1, vmin=120, vmax=370) # plot oxygen\n",
    "# ds_new.sigma0.plot.contour(y=\"PRES\", levels=85, colors='k', ax=ax, alpha=0.5, linewidths=0.5) # plot density anomalies\n",
    "\n",
    "ax.set_ylim([600, 0]); # set limits and **direction** of yaxis range\n",
    "# ax.set_xlim([np.datetime64(\"2007-07-01\"), np.datetime64(\"2008-07-01\")]); # how to set xlim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(20,5)) # prepare figure and axis objects\n",
    "ds_new.zm.plot(c='k', ax=ax, zorder=2) # plot zm\n",
    "ds_new.AOU.plot(y='PRES', cmap=get_cmap('BrBG'), ax=ax, zorder=1, vmin=-120, vmax=+120) # plot AOU \n",
    "CS = ds_new.sigma0.plot.contour(y=\"PRES\", levels=np.linspace(27, 29, 40), # this is to plot isopycnals\n",
    "                                colors='m', ax=ax, alpha=0.7, \n",
    "                                linewidths=1, \n",
    "                                kwargs=dict(inline=True),\n",
    "                               ) # plot density anomalies\n",
    "ax.clabel(CS, fmt=\"%.2f\")\n",
    "\n",
    "ax.set_ylim([300, 0]) # set limits and **direction** of yaxis range\n",
    "# ax.set_xlim([np.datetime64(\"2007-07-01\"), np.datetime64(\"2009-01-01\")]) # how to set xlim\n",
    "ax.grid('on', ls='--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Create fake AOU data to check if code works\n",
    "\n",
    "# ### fake AOU ####\n",
    "# ## constant AOU\n",
    "# # tmpAOU = np.empty(ds_new.AOU.shape) # create empy matrix\n",
    "# # tmpAOU[:] = 30. # fill with constant value\n",
    "# # ds_new.AOU.values = tmpAOU # assign to AOU DataArray\n",
    "\n",
    "# t = (ds_new.decimal_day.values - ds_new.decimal_day.values[0]) # decimal day array\n",
    "\n",
    "# # # linearly increasing AOU, but constant as a function of depth\n",
    "# # tmpAOU = np.empty(ds_new.AOU.shape) # create empy matrix\n",
    "# # for iaou in np.arange(tmpAOU.shape[0]):\n",
    "# #     tmpAOU[iaou,:] = 0 + 0.2 * np.tile(t[iaou], (ds_new.AOU.shape[1],1)).transpose()\n",
    "# # ds_new.AOU.values = tmpAOU # assign to AOU DataArray\n",
    "\n",
    "# # linearly increasing AOUwith time and as a function of depth\n",
    "# tmpAOU = np.empty(ds_new.AOU.shape) # create empy matrix\n",
    "\n",
    "# m = 0.2/(200 - 1000) # slope of vertical decrease in R\n",
    "# q = -m * 1000 # intecept of vertical decrease in R\n",
    "# R = (m*ds_new.PRES.values + q)\n",
    "# R[ds_new.PRES.values>1000] = 0\n",
    "\n",
    "# for iaou in np.arange(tmpAOU.shape[0]):\n",
    "#     tmpAOU[iaou,:] = 0 + R * np.tile(t[iaou], (ds_new.AOU.shape[1],1)).transpose()\n",
    "# ds_new.AOU.values = tmpAOU # assign to AOU DataArray\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Plot fake AOU data\n",
    "# ds_new.AOU.plot(x='JULD', yincrease=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Plot fake respiration data\n",
    "# fig, ax = plt.subplots(1)\n",
    "# ax.plot(R, ds_new.PRES.values)\n",
    "# ax.grid('on', ls='--')\n",
    "# ax.set_ylabel('PRES [dbar]')\n",
    "# ax.set_xlabel(\"R [umol/m3/d]\")\n",
    "# ax.set_ylim([2000, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Now create also fake sigma0 data\n",
    "# tmpSIGMA0 = np.linspace(np.nanmin(ds_new.sigma0.values), np.nanmax(ds_new.sigma0.values), ds_new.PRES.shape[0])\n",
    "# ds_new.sigma0.values = np.tile(tmpSIGMA0, (ds_new.sigma0.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### and plot them to check \n",
    "# ds_new.sigma0.plot(x='JULD', yincrease=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now plot data at given pressure levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of how to plot properties at a given PRES level\n",
    "PRES2PLOT = [50, 100, 200]\n",
    "\n",
    "fig, ax = plt.subplots(4,1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "ds_new.DOXY.sel(PRES=PRES2PLOT, \n",
    "                method='nearest'\n",
    "               ).plot.line( \n",
    "                        x='JULD',\n",
    "#                         marker='o', \n",
    "#                         markerfacecolor='w',\n",
    "                        ax=ax[0]\n",
    "                    )\n",
    "ax[0].grid('on', ls='--')\n",
    "ax[0].set_xlabel('')\n",
    "\n",
    "# ax[0].set_xlim(lims[0:2]); # set the xlim on the same range as the previous plot\n",
    "# ax[0].set_xlim([np.datetime64(\"2007-07-01\"), np.datetime64(\"2009-01-01\")]) # how to set xlim\n",
    "\n",
    "\n",
    "ds_new.AOU.sel(PRES=PRES2PLOT, \n",
    "                method='nearest'\n",
    "               ).plot.line(  \n",
    "                        x='JULD',\n",
    "                        ax=ax[1],\n",
    "#                         ylim=[0, +40],\n",
    "                    )\n",
    "ax[1].grid('on', ls='--')\n",
    "ax[1].set_title('')\n",
    "ax[1].set_xlabel('')\n",
    "# ax[1].invert_yaxis()\n",
    "ax[1].legend('')\n",
    "\n",
    "ds_new.TEMP.sel(PRES=PRES2PLOT, method='nearest'\n",
    "               ).plot.line(  \n",
    "                        x='JULD',\n",
    "                        ax=ax[2]\n",
    "                    )\n",
    "ax[2].set_title('')\n",
    "ax[2].set_xlabel('')\n",
    "ax[2].grid('on', ls='--')\n",
    "ax[2].legend('')\n",
    "\n",
    "ds_new.sigma0.sel(PRES=PRES2PLOT, method='nearest'\n",
    "               ).plot.line(  \n",
    "                        x='JULD',\n",
    "                        ax=ax[3],\n",
    "                    )\n",
    "ax[3].set_title('')\n",
    "ax[3].grid('on', ls='--')\n",
    "ax[3].legend('')\n",
    "ax[3].invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide mesopelagic in different pressure layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Dataset where all the values shallower than 200 dbar have been masked out (i.e., set equal to NaN) \n",
    "PRESmin = 50 # [dbar]\n",
    "PRESmax = (int(np.nanmax(ds_new.zp.values)/100) + 2)*100 # [dbar] based on deepest mixed layer \n",
    "PRESdelta = 20 # [dbar] thickness of each layer\n",
    "\n",
    "ds_new_R = ds_new.where((ds_new.PRES >= PRESmin) & (ds_new.PRES <= PRESmax))  \n",
    "\n",
    "# group by depth layers\n",
    "PRES_lvls = np.arange(PRESmin, PRESmax, PRESdelta) # levels used to divide the water colum in layers (need to be adapted to vertical resolution)\n",
    "PRES_lbls = np.diff(PRES_lvls)/2 + PRES_lvls[:-1] # Labels to use for each layer\n",
    "\n",
    "gb = ds_new_R.groupby_bins(group=ds_new_R.PRES, bins=PRES_lvls, \n",
    "                          right=True, labels=PRES_lbls, include_lowest=True, squeeze=True, restore_coord_dims=True)\n",
    "print(PRESmin, PRESmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gb` is a Python object that contains the resulting binned DataArrays for each layer. You can extract separate DataArrays by iterating over `gb` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for group_name, group_da in gb:\n",
    "#     print(\"group_name: \" + str(group_name))\n",
    "#     break # this is to stop at the first layer\n",
    "# group_da "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compute the mean value (of all variables) of the first PRES layer as a function of time.\n",
    "The example also shows how to plot the derived mean AOU values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=[10,2]) # prepare figure and axis for plotting\n",
    "il = PRES_lbls[0] # select the first PRES_bin label\n",
    "gb.mean(dim='PRES').AOU.sel(PRES_bins=il).plot(ax=ax, label=il, c='k') # compute and plot the mean AOU in the il layer\n",
    "(gb.mean(dim='PRES') + gb.std(dim='PRES')).AOU.sel(PRES_bins=il).plot(ax=ax, label=il, c='k', alpha=0.5) # add mean + 1 std\n",
    "(gb.mean(dim='PRES') - gb.std(dim='PRES')).AOU.sel(PRES_bins=il).plot(ax=ax, label=il, c='k', alpha=0.5) # add mean - 1 std\n",
    "\n",
    "### this is if you want to plot the mean at more than one layer\n",
    "# for il in PRES_lbls[:3]:\n",
    "#     gb.mean(dim='PRES').AOU.sel(PRES_bins=[il]).plot(ax=ax, label=il)\n",
    "#     ax.set_title('')\n",
    "# ax.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract one \"year\" of data \n",
    "To compute mesopelagic respiration, we now need to extract one year of data. Please select the year you want to plot by adding the start and stop dates. In the example below we use the shallowest layer, that is also the shortest in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_start = \"2007-09-14\"; date_stop = \"2008-10-01\"\n",
    "date_start = \"2016-10-01\"; date_stop = \"2017-05-01\"\n",
    "\n",
    "gb_yr_mean = gb.mean(dim='PRES').sel(JULD=slice(date_start, date_stop)) # this is the new group object with only the year we want\n",
    "gb_yr_std = gb.std(dim='PRES').sel(JULD=slice(date_start, date_stop)) # this is the new group object with only the year we want\n",
    "\n",
    "### to make sure we have the extracted the correct part of the dta, we plot the extrated subset\n",
    "fig, ax = plt.subplots(1,1, figsize=[10,4]) # prepare figure and axis for plotting\n",
    "il = PRES_lbls[0] # select the first PRES_bin label\n",
    "V = 'AOU'\n",
    "gb_yr_mean[V].sel(PRES_bins=il).plot(ax=ax, label=il, c='k', marker='o', lw=0) # compute and plot the mean AOU in the il layer\n",
    "(gb_yr_mean + gb_yr_std)[V].sel(PRES_bins=il).plot(ax=ax, label=il, c='k', lw=0.5, alpha=0.5) # compute and plot the mean AOU in the il layer\n",
    "(gb_yr_mean - gb_yr_std)[V].sel(PRES_bins=il).plot(ax=ax, label=il, c='k', lw=0.5, alpha=0.5) # compute and plot the mean AOU in the il layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate mesopelagic respiration by fitting AOU vs. time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### fit data and store results in dataframe\n",
    "\n",
    "# prepare figure for plotting fitted data\n",
    "fig, ax = plt.subplots(1,1, figsize=[6,8])\n",
    "\n",
    "# initialize array with slopes (i.e., R estimates)\n",
    "NN = len(gb_yr_mean.isel(PRES_bins=0).JULD.values)\n",
    "R = np.zeros(NN) + np.nan\n",
    "R_err = np.zeros(NN) + np.nan\n",
    "Rr = np.zeros(NN) + np.nan\n",
    "Rr_err = np.zeros(NN) + np.nan\n",
    "T = np.zeros(NN) + np.nan\n",
    "T_err = np.zeros(NN) + np.nan\n",
    "S = np.zeros(NN) + np.nan\n",
    "S_err = np.zeros(NN) + np.nan\n",
    "O2 = np.zeros(NN) + np.nan\n",
    "O2_err = np.zeros(NN) + np.nan\n",
    "P = np.zeros(NN) + np.nan\n",
    "P_err = np.zeros(NN) + np.nan\n",
    "\n",
    "for ilyr, lyr in enumerate(PRES_lbls):\n",
    "    \n",
    "    P[ilyr] = lyr\n",
    "    \n",
    "    # prepare data for fitting\n",
    "    x = gb_yr_mean.sel(PRES_bins=lyr).JULD.values # extract dates\n",
    "    x = x.astype('timedelta64[s]').astype(float)/(24 * 3600 * 1e9) # compute number of days from first day [1e9 is to convert from milliseconds]    \n",
    "    x = x - x[0]\n",
    "    y = gb_yr_mean.sel(PRES_bins=lyr).AOU.values\n",
    "    innan = np.where(~np.isnan(y) & ~np.isnan(x))[0]\n",
    "\n",
    "\n",
    "    \n",
    "    # fit the AOU data using both Ordinary Least Square (ols*) and a robust-fitting technique (rlm*) \n",
    "    ols, ols_result, rlm, rlm_result = bgc_tools.fit_linear(x[innan], y[innan]) # OLS: Ordinary Least Square; RLM: Robust fitting\n",
    "    \n",
    "    if ols_result.params[1] > 0:\n",
    "        if  ols_result.bse[1]/ols_result.params[1] < 2:\n",
    "            # store results\n",
    "            R[ilyr] = ols_result.params[1]\n",
    "            R_err[ilyr] = ols_result.bse[1]\n",
    "    \n",
    "    if rlm_result.params[1] > 0:\n",
    "        if  rlm_result.bse[1]/rlm_result.params[1] < 2:\n",
    "            # store results\n",
    "            Rr[ilyr] = rlm_result.params[1]\n",
    "            Rr_err[ilyr] = rlm_result.bse[1]\n",
    "            \n",
    "            # add plot for this layer\n",
    "            mrk, = ax.plot(x[innan], y[innan], 'o', label=f\"{P[ilyr]:.0f} dbar, {Rr[ilyr]:.3f} umol/kg/d\", alpha=0.3)\n",
    "            colr = mrk.get_color()\n",
    "    \n",
    "#             print(lyr, R[ilyr])\n",
    "    \n",
    "    ax.plot(x[innan], R[ilyr] * x[innan] + ols_result.params[0], '--', c=colr, alpha=0.3)  # \"R[ilyr] * x[innan]\"\" is \"slope * x\" which should give \"y\"\n",
    "    ax.plot(x[innan], Rr[ilyr] * x[innan] + rlm_result.params[0], '-', c=colr, alpha=0.3)  # \"R[ilyr] * x[innan]\"\" is \"slope * x\" which should give \"y\"\n",
    "\n",
    "\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5));\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #### print OLS results\n",
    "# print(R)\n",
    "# print(R_err/R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #### print robust-fitting results\n",
    "print(Rr)\n",
    "print(Rr_err/Rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to mmolC/m-3/yr\n",
    "Note that to also convert the \"error\" we use the [Standard Law of Propagation of Uncertainty (SLPU)](https://www.bipm.org/documents/20126/2071204/JCGM_100_2008_E.pdf/cb0ef43f-baa5-11cf-3f85-4dcd86f77bd6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to mmolC/m3/yr\n",
    "RrC, RrC_err = bgc_tools.umolO2_kg_d_TO_mmolC_m3_yr(Rr, R_ERR=Rr_err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot profile of respiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=[3,4])\n",
    "\n",
    "ax.errorbar(RrC, P, fmt='bs', yerr=P_err, xerr=2*RrC_err, alpha=0.25, elinewidth=3, label='robust')\n",
    "ax.set_ylim([PRESmax+100, PRESmin-50])\n",
    "ax.grid('on', ls='--', c='lightgrey')\n",
    "ax.set_xlabel('R [mmolC/m3/yr]')\n",
    "ax.set_ylabel('PRES [dbar]')\n",
    "# ax.set_xlim([0, 20])\n",
    "ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data as a function of density, rather than pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,3))\n",
    "X = np.tile(ds_new.JULD.values, [ds_new.PRES.shape[0],1]).transpose()\n",
    "# Y = np.tile(ds_new.PRES.values, [ds_new.JULD.shape[0],1]) # if you want to plot as a function of sigma0\n",
    "Y = ds_new.sigma0.values # if you want to plot as a function of PRES\n",
    "h = ax.scatter(X, Y, c=ds_new.AOU.values, cmap=get_cmap('BrBG'), vmin=-80, vmax=+80)\n",
    "ax.plot(ds_new.JULD.values, ds_new.zm_sigma0.values, 'k')\n",
    "ax.set_ylim([28, 26.5])\n",
    "# create colorbar\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"2%\", pad=0.05)\n",
    "plt.colorbar(h, cax=cax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide mesopelagic in different density layers\n",
    "Because sigma is not a dimension or coordinate in the xarray Dataset `ds_new`, we need to proceed in a different way than we did when we divided the water colum in pressure layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### select year (best decided based on the top-most layer)\n",
    "ds_new_yr = ds_new.sel(JULD=slice(date_start, date_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid getting into the productive layer, create a new Dataset where all the values shallower than 200 dbar have been masked out (i.e., set equal to NaN) \n",
    "SIGMAmin = np.round(np.nanmin(ds_new_yr.sel(PRES=150).sigma0)*10)/10 # [kg/m3] extract the lowest sigma0 deeper than PRESmin and then round it up\n",
    "SIGMAmax = np.ceil(np.nanmax(ds_new_yr.zm_sigma0.values)) # [kg/m3] find largest sigma0 corresponding to mixed layer \n",
    "SIGMAdelta = 0.01 # [dbar] thickness of each layer\n",
    "\n",
    "ds_new_R_sigma = ds_new.where((ds_new_yr.sigma0 >= SIGMAmin) & (ds_new_yr.sigma0 <= SIGMAmax))  # create new dataset wth data within the sigma layers defined above\n",
    "\n",
    "##### bin by depth layers\n",
    "# define bins and labels\n",
    "SIGMA_bins = np.arange(SIGMAmin, SIGMAmax, SIGMAdelta) # bins used to divide the water colum in layers (need to be adapted to vertical resolution)\n",
    "SIGMA_lbls = np.diff(SIGMA_bins)/2 + SIGMA_bins[:-1] # Labels to use for each layer\n",
    "\n",
    "# bin data\n",
    "gb_sigma = ds_new_R_sigma.groupby_bins(group=ds_new_R_sigma.sigma0, bins=SIGMA_bins, \n",
    "                          right=True, labels=SIGMA_lbls, include_lowest=True, squeeze=True, restore_coord_dims=True)\n",
    "\n",
    "print(SIGMAmin, SIGMAmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGMA_lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,3))\n",
    "X = np.tile(ds_new_R_sigma.JULD.values, [ds_new_R_sigma.PRES.shape[0],1]).transpose()\n",
    "Y = ds_new_R_sigma.sigma0.values # if you want to plot as a function of PRES\n",
    "h = ax.scatter(X, Y, c=ds_new_R_sigma.AOU.values, s=10, cmap=get_cmap('BrBG'))#, vmin=-80, vmax=+80)\n",
    "ax.plot(ds_new_R_sigma.JULD.values, ds_new_R_sigma.zm_sigma0.values, 'k')\n",
    "ax.set_ylim([28, 27])\n",
    "# create colorbar\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"2%\", pad=0.05)\n",
    "plt.colorbar(h, cax=cax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because sigma is not a coordinate in the Xarray Dataset, we need to fit the data group by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for group_name, group_da in gb_sigma:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prepare figure for plotting fitted data\n",
    "fig, ax = plt.subplots(1,1, figsize=[6,4])\n",
    "\n",
    "# initialize array with slopes (i.e., R estimates)\n",
    "NN = len(SIGMA_lbls)\n",
    "R_sigma = np.zeros(NN) + np.nan\n",
    "R_err_sigma = np.zeros(NN) + np.nan\n",
    "Rr_sigma = np.zeros(NN) + np.nan\n",
    "Rr_err_sigma = np.zeros(NN) + np.nan\n",
    "T_sigma = np.zeros(NN) + np.nan\n",
    "T_err_sigma = np.zeros(NN) + np.nan\n",
    "S_sigma = np.zeros(NN) + np.nan\n",
    "S_err_sigma = np.zeros(NN) + np.nan\n",
    "O2_sigma = np.zeros(NN) + np.nan\n",
    "O2_err_sigma = np.zeros(NN) + np.nan\n",
    "P_sigma = np.zeros(NN) + np.nan\n",
    "P_err_sigma = np.zeros(NN) + np.nan\n",
    "\n",
    "isigma = 0 # index used to fill arrays with results\n",
    "\n",
    "\n",
    "MAXrelerr = 2 # max relative error accepted\n",
    "\n",
    "for group_name, group_da in gb_sigma:\n",
    "#     print(\"group_name: \" + str(group_name))\n",
    "    \n",
    "    # extract PRES and its variability from this layer\n",
    "    P_sigma[isigma] = np.nanmean(group_da.PRES)\n",
    "    P_err_sigma[isigma] = np.nanstd(group_da.PRES)\n",
    "    \n",
    "    # prepare data for fitting\n",
    "    x = group_da.JULD.values # extract dates\n",
    "    x = x.astype('timedelta64[s]').astype(float)/(24 * 3600 * 1e9) # compute number of days from first day [1e9 is to convert from milliseconds]    \n",
    "    x = x - x[0]\n",
    "    y = group_da.AOU.values\n",
    "    innan = np.where(~np.isnan(y) & ~np.isnan(x))[0]\n",
    "\n",
    "    \n",
    "    # fit the AOU data using both Ordinary Least Square (ols*) and a robust-fitting technique (rlm*) \n",
    "    ols, ols_result, rlm, rlm_result = bgc_tools.fit_linear(x[innan], y[innan]) # OLS: Ordinary Least Square; RLM: Robust fitting\n",
    "    \n",
    "    if ols_result.params[1] > 0:\n",
    "        if  ols_result.bse[1]/ols_result.params[1] < MAXrelerr:\n",
    "            # store results\n",
    "            R_sigma[isigma] = ols_result.params[1]\n",
    "            R_err_sigma[isigma] = ols_result.bse[1]\n",
    "    \n",
    "    if rlm_result.params[1] > 0:\n",
    "        if  rlm_result.bse[1]/rlm_result.params[1] < MAXrelerr:\n",
    "            # store results\n",
    "            Rr_sigma[isigma] = rlm_result.params[1]\n",
    "            Rr_err_sigma[isigma] = rlm_result.bse[1]\n",
    "            \n",
    "            # plot some of the fits\n",
    "            if isigma < 40:\n",
    "                stride = 1\n",
    "                if isigma/stride == int(isigma/stride):\n",
    "                    # add plot for this layer\n",
    "                    mrk, = ax.plot(x[innan], y[innan], 'o', label=f\"{group_name:.2f} kg/m3, {P_sigma[isigma]:.0f} dbar, {Rr_sigma[isigma]:.3f} umol/kg/d\", alpha=0.3)\n",
    "                    colr = mrk.get_color()\n",
    "                    ax.plot(x[innan], Rr_sigma[isigma] * x[innan] + rlm_result.params[0], '-', c=colr, alpha=0.3)  # \"R[ilyr] * x[innan]\"\" is \"slope * x\" which should give \"y\"\n",
    "\n",
    "    isigma = isigma + 1 # increment index\n",
    "\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5));\n",
    "ax.set_xlabel(\"days\")\n",
    "ax.set_ylabel(\"AOU [umol/kg]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate mesopelagic respiration by fitting AOU vs. time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #### print robust-fitting results\n",
    "print(Rr_sigma)\n",
    "Rr_err_sigma / Rr_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to mmolC/m-3/yr\n",
    "Note that to also convert the \"error\" we use the [Standard Law of Propagation of Uncertainty (SLPU)](https://www.bipm.org/documents/20126/2071204/JCGM_100_2008_E.pdf/cb0ef43f-baa5-11cf-3f85-4dcd86f77bd6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to mmolC/m3/yr\n",
    "RrC_sigma, RrC_err_sigma = bgc_tools.umolO2_kg_d_TO_mmolC_m3_yr(Rr_sigma, R_ERR=Rr_err_sigma) # robust-fit estimates\n",
    "\n",
    "RC_sigma, RC_err_sigma = bgc_tools.umolO2_kg_d_TO_mmolC_m3_yr(R_sigma, R_ERR=R_err_sigma) # ols-fit estimates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot profile of respiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=[3,4])\n",
    "\n",
    "ax.errorbar(RrC_sigma, P_sigma, fmt='bs', \n",
    "            yerr=P_err_sigma, xerr=2*RrC_err_sigma, \n",
    "            alpha=0.25, elinewidth=3, label='robust')\n",
    "# ax.errorbar(RC_sigma, P_sigma, fmt='ro', yerr=P_err_sigma, xerr=2*RC_err_sigma, alpha=0.25, elinewidth=3, label='ols')\n",
    "ax.set_ylim([PRESmax+100, PRESmin-50])\n",
    "ax.grid('on', ls='--', c='lightgrey')\n",
    "ax.set_xlabel('R [mmolC/m$^3$/yr]')\n",
    "ax.set_ylabel('PRES [dbar]')\n",
    "# ax.set_xlim([0, 20])\n",
    "ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### this is to write out the libraries and hardware used to run the original version of this Notebook\n",
    "%load_ext watermark\n",
    "%watermark -v -m -p xarray,numpy,scipy,pandas,glob,matplotlib,hvplot,pdb,cmocean,matplotlib,gsw,sys,cartopy,statsmodels\n",
    "\n",
    "print(\" \")\n",
    "%watermark -u -n -t -z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
